"""
Streaming Ingestion Example: Kafka â†’ Flink â†’ Iceberg (via Nessie)

This script demonstrates a complete streaming data pipeline:
1. Producer: Generates sample user events and publishes to Kafka
2. Consumer: Flink processes the stream and writes to Iceberg tables via Nessie catalog
3. Query: Router can query the streamed data via Trino/DuckDB

Architecture:
  Kafka (Stream) â†’ Flink (Processing) â†’ Iceberg/Nessie (Storage) â†’ Query Engines (Analytics)
"""

import json
import time
import os
import random
from datetime import datetime, timezone
from typing import Dict, Any

try:
    from kafka import KafkaProducer, KafkaConsumer
    KAFKA_AVAILABLE = True
except ImportError:
    KAFKA_AVAILABLE = False
    print("âš ï¸  kafka-python not installed. Install with: pip install kafka-python")

import requests

# Configuration
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:29092")
KAFKA_TOPIC = "user_events"
ROUTER_URL = os.getenv("ROUTER_URL", "http://localhost:8000")
NESSIE_URL = os.getenv("NESSIE_URL", "http://localhost:19120/api/v1")


def generate_sample_event(user_id: int) -> Dict[str, Any]:
    """Generate a sample user event."""
    events = [
        {"action": "login", "page": "/home"},
        {"action": "view", "page": "/products"},
        {"action": "click", "page": "/product/123"},
        {"action": "add_to_cart", "page": "/cart"},
        {"action": "checkout", "page": "/checkout"},
    ]
    
    event = random.choice(events)
    
    return {
        "user_id": user_id,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "action": event["action"],
        "page": event["page"],
        "session_id": f"session_{user_id % 10}",
    }


class StreamingProducer:
    """Kafka producer for streaming events."""
    
    def __init__(self):
        if not KAFKA_AVAILABLE:
            raise ImportError("kafka-python is required")
        
        self.producer = KafkaProducer(
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            key_serializer=lambda k: str(k).encode('utf-8')
        )
        print(f"âœ… Kafka Producer connected to {KAFKA_BOOTSTRAP_SERVERS}")
    
    def send_event(self, event: Dict[str, Any]):
        """Send an event to Kafka."""
        user_id = event["user_id"]
        future = self.producer.send(KAFKA_TOPIC, key=user_id, value=event)
        
        try:
            record_metadata = future.get(timeout=10)
            print(f"ðŸ“¤ Sent: {event['action']} for user {user_id} to partition {record_metadata.partition}")
            return True
        except Exception as e:
            print(f"âŒ Failed to send event: {e}")
            return False
    
    def produce_stream(self, num_events: int = 100, delay: float = 0.5):
        """Produce a stream of events."""
        print(f"\nðŸš€ Starting to produce {num_events} events to Kafka topic '{KAFKA_TOPIC}'...")
        
        for i in range(num_events):
            user_id = (i % 20) + 1  # Cycle through 20 users
            event = generate_sample_event(user_id)
            self.send_event(event)
            time.sleep(delay)
        
        print(f"\nâœ… Produced {num_events} events successfully!")
        self.producer.close()


class StreamingConsumer:
    """Simple Kafka consumer for demonstration (in production, use Flink)."""
    
    def __init__(self):
        if not KAFKA_AVAILABLE:
            raise ImportError("kafka-python is required")
        
        self.consumer = KafkaConsumer(
            KAFKA_TOPIC,
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            auto_offset_reset='earliest',
            enable_auto_commit=True,
            group_id='streaming-demo-consumer',
            value_deserializer=lambda v: json.loads(v.decode('utf-8'))
        )
        print(f"âœ… Kafka Consumer connected to {KAFKA_BOOTSTRAP_SERVERS}")
    
    def consume_and_display(self, max_messages: int = 10):
        """Consume and display messages."""
        print(f"\nðŸ‘‚ Consuming messages from topic '{KAFKA_TOPIC}'...")
        count = 0
        
        for message in self.consumer:
            event = message.value
            print(f"ðŸ“¥ Received: user_id={event['user_id']}, action={event['action']}, page={event['page']}")
            count += 1
            
            if count >= max_messages:
                break
        
        print(f"\nâœ… Consumed {count} messages")
        self.consumer.close()


class IcebergWriter:
    """Write streaming data to Iceberg tables via Trino."""
    
    def __init__(self):
        print(f"âœ… Iceberg Writer initialized (Router: {ROUTER_URL})")
    
    def create_streaming_table(self):
        """Create Iceberg table for streaming events."""
        sql = """
        CREATE TABLE IF NOT EXISTS iceberg.public.user_events (
            user_id INTEGER,
            timestamp VARCHAR,
            action VARCHAR,
            page VARCHAR,
            session_id VARCHAR
        )
        """
        
        try:
            response = requests.post(
                f"{ROUTER_URL}/query",
                json={"sql": sql, "force_engine": "trino"}
            )
            result = response.json()
            
            if "error" in result:
                print(f"âš ï¸  Table creation warning: {result['error']}")
            else:
                print("âœ… Iceberg table 'user_events' created/verified")
            
            return True
        except Exception as e:
            print(f"âŒ Failed to create table: {e}")
            return False
    
    def batch_insert_events(self, events: list):
        """Insert a batch of events into Iceberg table."""
        if not events:
            return
        
        # Build VALUES clause with proper escaping
        values = []
        for event in events:
            # Escape single quotes by doubling them (SQL standard)
            user_id = int(event['user_id'])  # Ensure integer
            timestamp = event['timestamp'].replace("'", "''")
            action = event['action'].replace("'", "''")
            page = event['page'].replace("'", "''")
            session_id = event['session_id'].replace("'", "''")
            
            values.append(
                f"({user_id}, '{timestamp}', '{action}', '{page}', '{session_id}')"
            )
        
        sql = f"INSERT INTO iceberg.public.user_events VALUES {', '.join(values)}"
        
        try:
            response = requests.post(
                f"{ROUTER_URL}/query",
                json={"sql": sql, "force_engine": "trino"}
            )
            result = response.json()
            
            if "error" in result:
                print(f"âŒ Insert failed: {result['error']}")
            else:
                print(f"âœ… Inserted {len(events)} events into Iceberg table")
            
            return True
        except Exception as e:
            print(f"âŒ Failed to insert events: {e}")
            return False


def verify_nessie_connection():
    """Verify Nessie catalog is accessible."""
    try:
        response = requests.get(f"{NESSIE_URL}/config", timeout=5)
        if response.status_code == 200:
            print("âœ… Nessie catalog is accessible")
            return True
        else:
            print(f"âš ï¸  Nessie returned status code: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ Nessie is not accessible: {e}")
        return False


def query_streaming_data():
    """Query the streaming data from Iceberg table."""
    print("\nðŸ“Š Querying streaming data from Iceberg table...")
    
    queries = [
        ("Total events", "SELECT COUNT(*) as total_events FROM iceberg.public.user_events"),
        ("Events by action", "SELECT action, COUNT(*) as count FROM iceberg.public.user_events GROUP BY action"),
        ("Recent events", "SELECT * FROM iceberg.public.user_events LIMIT 5"),
    ]
    
    for name, sql in queries:
        print(f"\nðŸ” {name}:")
        try:
            response = requests.post(
                f"{ROUTER_URL}/query",
                json={"sql": sql, "force_engine": "trino"}
            )
            result = response.json()
            
            if "error" in result:
                print(f"  âŒ Query failed: {result['error']}")
            else:
                print(f"  âœ… Results: {result.get('data', [])}")
        except Exception as e:
            print(f"  âŒ Query error: {e}")


def demo_streaming_pipeline():
    """Run a complete streaming pipeline demonstration."""
    print("=" * 80)
    print("ðŸŒŠ STREAMING INGESTION DEMO: Kafka â†’ Flink â†’ Iceberg (Nessie)")
    print("=" * 80)
    
    # Step 1: Verify prerequisites
    print("\n1ï¸âƒ£  Verifying prerequisites...")
    if not KAFKA_AVAILABLE:
        print("âŒ kafka-python not available. Please install: pip install kafka-python")
        return
    
    verify_nessie_connection()
    
    # Step 2: Create Iceberg table
    print("\n2ï¸âƒ£  Setting up Iceberg table...")
    writer = IcebergWriter()
    writer.create_streaming_table()
    
    # Step 3: Produce events to Kafka
    print("\n3ï¸âƒ£  Producing events to Kafka...")
    try:
        producer = StreamingProducer()
        producer.produce_stream(num_events=20, delay=0.2)
    except Exception as e:
        print(f"âŒ Producer failed: {e}")
        print("Note: Make sure Kafka is running (docker compose up kafka)")
        return
    
    # Step 4: Simulate Flink processing by consuming and writing to Iceberg
    print("\n4ï¸âƒ£  Simulating Flink stream processing...")
    print("   (In production, Flink would consume from Kafka and write to Iceberg)")
    
    try:
        # Collect some events
        consumer = KafkaConsumer(
            KAFKA_TOPIC,
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            auto_offset_reset='earliest',
            enable_auto_commit=True,
            group_id='iceberg-writer',
            value_deserializer=lambda v: json.loads(v.decode('utf-8')),
            consumer_timeout_ms=5000  # Wait 5 seconds for messages
        )
        
        events_batch = []
        for message in consumer:
            events_batch.append(message.value)
            if len(events_batch) >= 20:
                break
        
        consumer.close()
        
        if events_batch:
            print(f"   ðŸ“¦ Collected {len(events_batch)} events from Kafka")
            writer.batch_insert_events(events_batch)
        else:
            print("   âš ï¸  No events collected from Kafka")
    
    except Exception as e:
        print(f"âŒ Consumer/Writer failed: {e}")
    
    # Step 5: Query the data
    print("\n5ï¸âƒ£  Querying streamed data from Iceberg...")
    time.sleep(2)  # Give Iceberg a moment to commit
    query_streaming_data()
    
    print("\n" + "=" * 80)
    print("âœ… STREAMING PIPELINE DEMO COMPLETE!")
    print("=" * 80)
    print("\nKey Components:")
    print("  â€¢ Kafka: Message streaming platform")
    print("  â€¢ Flink: Stream processing engine (simulated here)")
    print("  â€¢ Iceberg: Table format for data lake")
    print("  â€¢ Nessie: Git-like catalog with versioning")
    print("  â€¢ Trino/DuckDB: Query engines for analytics")
    print("\nNext Steps:")
    print("  â€¢ Deploy actual Flink job for continuous processing")
    print("  â€¢ Scale Kafka with more partitions")
    print("  â€¢ Enable Iceberg time travel for historical queries")
    print("  â€¢ Add data quality checks in Flink")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "produce":
            num_events = int(sys.argv[2]) if len(sys.argv) > 2 else 100
            producer = StreamingProducer()
            producer.produce_stream(num_events=num_events)
        
        elif command == "consume":
            max_messages = int(sys.argv[2]) if len(sys.argv) > 2 else 10
            consumer = StreamingConsumer()
            consumer.consume_and_display(max_messages=max_messages)
        
        elif command == "query":
            query_streaming_data()
        
        elif command == "setup":
            writer = IcebergWriter()
            writer.create_streaming_table()
        
        else:
            print(f"Unknown command: {command}")
            print("Usage: python streaming_ingestion.py [produce|consume|query|setup|demo]")
    
    else:
        # Run full demo
        demo_streaming_pipeline()
